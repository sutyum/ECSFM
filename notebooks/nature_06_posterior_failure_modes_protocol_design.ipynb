{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment: Nature Evidence 06 - Posterior Failure Modes and Protocol Design\n",
        "\n",
        "Objective:\n",
        "- Stress-test posterior inference under model mismatch (biofouled multiphysics traces outside base training assumptions).\n",
        "- Demonstrate that reliability diagnostics can detect out-of-family measurements.\n",
        "- Provide an operational guardrail for deployment: when to trust inference and when to escalate.\n",
        "\n",
        "Success criteria:\n",
        "- In-distribution and stress-test reliability distributions separate measurably.\n",
        "- A threshold-based rule achieves strong stress-case recall with controlled false positives.\n",
        "- Representative failure traces are visualized and documented.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Setup: imports and reproducibility\n",
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import equinox as eqx\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from ecsfm.fm.eval_classical import _resolve_model_geometry\n",
        "from ecsfm.fm.model import VectorFieldNet\n",
        "from ecsfm.fm.posterior import CEMPosteriorConfig, PosteriorInferenceConfig, infer_parameter_posterior\n",
        "from ecsfm.fm.train import (\n",
        "    MODEL_META_FILENAME,\n",
        "    NORMALIZERS_FILENAME,\n",
        "    load_model_metadata,\n",
        "    load_saved_normalizers,\n",
        ")\n",
        "from ecsfm.sim.multiphysics import MultiPhysicsConfig, simulate_multiphysics_electrochem\n",
        "\n",
        "np.random.seed(2026)\n",
        "\n",
        "ARTIFACT_DIR = Path('/tmp/ecsfm/notebook_nature_06')\n",
        "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print(f'Artifacts: {ARTIFACT_DIR}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plan\n",
        "\n",
        "- Build paired evaluation cases: in-distribution traces and biofouled multiphysics stress traces.\n",
        "- Run posterior inference with identical settings on both sets.\n",
        "- Compare reliability/calibration behavior and fit a simple acceptance rule.\n",
        "- Visualize one representative stress-case mismatch.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# User configuration and model/data loading\n",
        "def _first_existing(candidates: list[Path]) -> Path | None:\n",
        "    for candidate in candidates:\n",
        "        if candidate.exists():\n",
        "            return candidate\n",
        "    return None\n",
        "\n",
        "\n",
        "def _discover_latest(pattern: str, roots: list[Path]) -> Path | None:\n",
        "    found: list[Path] = []\n",
        "    for root in roots:\n",
        "        if root.exists():\n",
        "            found.extend(root.rglob(pattern))\n",
        "    if not found:\n",
        "        return None\n",
        "    return max(found, key=lambda p: p.stat().st_mtime)\n",
        "\n",
        "\n",
        "checkpoint_override = os.getenv('ECSFM_CHECKPOINT')\n",
        "dataset_override = os.getenv('ECSFM_DATASET_CHUNK')\n",
        "\n",
        "checkpoint_candidates = [\n",
        "    Path('/tmp/ecsfm/fullscale_balanced_modal/surrogate_model.eqx'),\n",
        "    Path('/vol/artifacts/fullscale_balanced_modal/surrogate_model.eqx'),\n",
        "    Path('/tmp/ecsfm/surrogate_model.eqx'),\n",
        "]\n",
        "dataset_candidates = [\n",
        "    Path('/tmp/ecsfm/dataset_balanced_742k/chunk_0.npz'),\n",
        "    Path('/vol/datasets/dataset_balanced_742k/chunk_0.npz'),\n",
        "    Path('/tmp/ecsfm/dataset_massive/chunk_0.npz'),\n",
        "]\n",
        "\n",
        "if checkpoint_override:\n",
        "    CHECKPOINT = Path(checkpoint_override)\n",
        "else:\n",
        "    CHECKPOINT = _first_existing(checkpoint_candidates)\n",
        "    if CHECKPOINT is None:\n",
        "        CHECKPOINT = _discover_latest('surrogate_model.eqx', [Path('/tmp/ecsfm'), Path('/vol/artifacts')])\n",
        "\n",
        "if dataset_override:\n",
        "    DATASET_CHUNK = Path(dataset_override)\n",
        "else:\n",
        "    DATASET_CHUNK = _first_existing(dataset_candidates)\n",
        "    if DATASET_CHUNK is None:\n",
        "        DATASET_CHUNK = _discover_latest('chunk_0.npz', [Path('/tmp/ecsfm'), Path('/vol/datasets')])\n",
        "    if DATASET_CHUNK is None:\n",
        "        DATASET_CHUNK = _discover_latest('chunk_*.npz', [Path('/tmp/ecsfm'), Path('/vol/datasets')])\n",
        "\n",
        "if CHECKPOINT is None or not CHECKPOINT.exists():\n",
        "    raise FileNotFoundError('Checkpoint not found. Set ECSFM_CHECKPOINT.')\n",
        "if DATASET_CHUNK is None or not DATASET_CHUNK.exists():\n",
        "    raise FileNotFoundError('Dataset chunk not found. Set ECSFM_DATASET_CHUNK.')\n",
        "\n",
        "N_CASES = 6\n",
        "EVAL_NOISE_STD = 0.05\n",
        "\n",
        "INFERENCE_CFG = {\n",
        "    'n_particles': 64,\n",
        "    'n_iters': 4,\n",
        "    'elite_frac': 0.25,\n",
        "    'n_mc': 2,\n",
        "    'n_steps': 60,\n",
        "}\n",
        "\n",
        "normalizers = load_saved_normalizers(CHECKPOINT.parent / NORMALIZERS_FILENAME)\n",
        "meta = load_model_metadata(CHECKPOINT.parent / MODEL_META_FILENAME)\n",
        "geometry = _resolve_model_geometry(normalizers, meta)\n",
        "\n",
        "key = jax.random.PRNGKey(np.uint32(2026))\n",
        "_, model_key = jax.random.split(key)\n",
        "model = VectorFieldNet(\n",
        "    state_dim=int(geometry['state_dim']),\n",
        "    hidden_size=int(meta.get('hidden_size', 128)),\n",
        "    depth=int(meta.get('depth', 3)),\n",
        "    cond_dim=int(meta.get('cond_dim', 32)),\n",
        "    phys_dim=int(geometry['phys_dim']),\n",
        "    signal_channels=int(geometry['signal_channels']),\n",
        "    key=model_key,\n",
        ")\n",
        "model = eqx.tree_deserialise_leaves(CHECKPOINT, model)\n",
        "\n",
        "with np.load(DATASET_CHUNK) as chunk:\n",
        "    currents = np.asarray(chunk['i'], dtype=np.float32)\n",
        "    signals = np.asarray(chunk['e'], dtype=np.float32)\n",
        "    params_base = np.asarray(chunk['p'], dtype=np.float32)\n",
        "    task_ids = np.asarray(chunk['task_id'], dtype=np.int32)\n",
        "    stage_ids = np.asarray(chunk['stage_id'], dtype=np.int32)\n",
        "\n",
        "rng = np.random.default_rng(2028)\n",
        "case_indices = np.asarray(rng.choice(currents.shape[0], size=min(N_CASES, currents.shape[0]), replace=False), dtype=np.int32)\n",
        "\n",
        "phys_dim_base = int(geometry['phys_dim_base'])\n",
        "phys_dim_core = int(geometry['phys_dim_core'])\n",
        "n_tasks = int(geometry['n_tasks'])\n",
        "n_stages = int(geometry['n_stages'])\n",
        "max_species = int(geometry['max_species'])\n",
        "\n",
        "\n",
        "def compose_core_params(base_row: np.ndarray, task_idx: int, stage_idx: int) -> np.ndarray:\n",
        "    out = np.zeros((phys_dim_core,), dtype=np.float32)\n",
        "    out[:phys_dim_base] = base_row[:phys_dim_base]\n",
        "    cursor = phys_dim_base\n",
        "    if n_tasks > 0:\n",
        "        onehot = np.zeros((n_tasks,), dtype=np.float32)\n",
        "        onehot[int(np.clip(task_idx, 0, n_tasks - 1))] = 1.0\n",
        "        out[cursor : cursor + n_tasks] = onehot\n",
        "        cursor += n_tasks\n",
        "    if n_stages > 0:\n",
        "        onehot = np.zeros((n_stages,), dtype=np.float32)\n",
        "        onehot[int(np.clip(stage_idx, 0, n_stages - 1))] = 1.0\n",
        "        out[cursor : cursor + n_stages] = onehot\n",
        "    return out\n",
        "\n",
        "\n",
        "p_core_true = np.stack(\n",
        "    [compose_core_params(params_base[i], int(task_ids[i]), int(stage_ids[i])) for i in case_indices],\n",
        "    axis=0,\n",
        ")\n",
        "\n",
        "_, _, _, _, p_mean, p_std = normalizers\n",
        "p_mean = np.asarray(p_mean, dtype=np.float32)\n",
        "p_std = np.asarray(p_std, dtype=np.float32)\n",
        "\n",
        "print('cases:', case_indices.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Build stress-test observations from multiphysics biofouling dynamics\n",
        "def decode_base_params(base_row: np.ndarray, m: int) -> dict[str, np.ndarray]:\n",
        "    row = np.asarray(base_row, dtype=np.float32)\n",
        "    return {\n",
        "        'D_ox': np.exp(row[0:m]),\n",
        "        'D_red': np.exp(row[m : 2 * m]),\n",
        "        'C_ox': np.clip(row[2 * m : 3 * m], 1e-5, None),\n",
        "        'C_red': np.clip(row[3 * m : 4 * m], 0.0, None),\n",
        "        'E0': row[4 * m : 5 * m],\n",
        "        'k0': np.exp(row[5 * m : 6 * m]),\n",
        "        'alpha': np.clip(row[6 * m : 7 * m], 0.05, 0.95),\n",
        "    }\n",
        "\n",
        "\n",
        "def simulate_biofouled_current(signal: np.ndarray, base_row: np.ndarray) -> np.ndarray:\n",
        "    p = decode_base_params(base_row, max_species)\n",
        "    cfg = MultiPhysicsConfig(\n",
        "        initial_theta=0.78,\n",
        "        k_ads=4200.0,\n",
        "        k_des=8.0e-5,\n",
        "        k_clean=0.02,\n",
        "        k_reaction=0.004,\n",
        "        Rfilm_theta_max_ohm=1800.0,\n",
        "        cdl_theta_fraction=0.82,\n",
        "        area_floor_fraction=0.09,\n",
        "        k0_theta_coeff=2.8,\n",
        "        electrode_area_cm2=0.01,\n",
        "    )\n",
        "    out = simulate_multiphysics_electrochem(\n",
        "        E_array=jnp.asarray(signal, dtype=jnp.float32),\n",
        "        t_max=8.0,\n",
        "        nx=24,\n",
        "        config=cfg,\n",
        "        D_ox=jnp.asarray(p['D_ox'], dtype=jnp.float32),\n",
        "        D_red=jnp.asarray(p['D_red'], dtype=jnp.float32),\n",
        "        C_bulk_ox=jnp.asarray(p['C_ox'], dtype=jnp.float32),\n",
        "        C_bulk_red=jnp.asarray(p['C_red'], dtype=jnp.float32),\n",
        "        E0=jnp.asarray(p['E0'], dtype=jnp.float32),\n",
        "        k0=jnp.asarray(p['k0'], dtype=jnp.float32),\n",
        "        alpha=jnp.asarray(p['alpha'], dtype=jnp.float32),\n",
        "        save_every=0,\n",
        "    )\n",
        "    return np.asarray(out[4], dtype=np.float32)\n",
        "\n",
        "\n",
        "def random_mask(length: int, keep_prob: float, rng: np.random.Generator, min_points: int = 24) -> np.ndarray:\n",
        "    mask = rng.random(length) < keep_prob\n",
        "    if int(mask.sum()) < min_points:\n",
        "        idx = rng.choice(length, size=min(min_points, length), replace=False)\n",
        "        mask[idx] = True\n",
        "    return mask.astype(np.float32)\n",
        "\n",
        "\n",
        "known_mask = np.zeros((phys_dim_core,), dtype=bool)\n",
        "known_mask[phys_dim_base:] = True\n",
        "\n",
        "posterior_cfg = PosteriorInferenceConfig(\n",
        "    cem=CEMPosteriorConfig(\n",
        "        n_particles=int(INFERENCE_CFG['n_particles']),\n",
        "        n_iterations=int(INFERENCE_CFG['n_iters']),\n",
        "        elite_fraction=float(INFERENCE_CFG['elite_frac']),\n",
        "    ),\n",
        "    n_mc_per_particle=int(INFERENCE_CFG['n_mc']),\n",
        "    n_integration_steps=int(INFERENCE_CFG['n_steps']),\n",
        "    obs_noise_std=float(EVAL_NOISE_STD),\n",
        ")\n",
        "\n",
        "runs: list[dict[str, float | str | int]] = []\n",
        "examples: dict[str, dict[str, np.ndarray | float | int]] = {}\n",
        "\n",
        "for local_idx, row_idx in enumerate(case_indices):\n",
        "    rng_case = np.random.default_rng(5000 + int(row_idx))\n",
        "    signal = np.asarray(signals[row_idx], dtype=np.float32)\n",
        "    base_row = np.asarray(params_base[row_idx], dtype=np.float32)\n",
        "\n",
        "    id_current = np.asarray(currents[row_idx], dtype=np.float32)\n",
        "    ood_current = simulate_biofouled_current(signal, base_row)\n",
        "\n",
        "    id_obs = id_current + rng_case.normal(0.0, EVAL_NOISE_STD, size=id_current.shape[0]).astype(np.float32)\n",
        "    ood_obs = ood_current + rng_case.normal(0.0, EVAL_NOISE_STD, size=ood_current.shape[0]).astype(np.float32)\n",
        "\n",
        "    id_mask = random_mask(id_obs.shape[0], keep_prob=0.40, rng=rng_case)\n",
        "    ood_mask = random_mask(ood_obs.shape[0], keep_prob=0.40, rng=rng_case)\n",
        "\n",
        "    for split_name, obs_current, obs_mask in (\n",
        "        ('in_distribution', id_obs, id_mask),\n",
        "        ('stress_biofouled', ood_obs, ood_mask),\n",
        "    ):\n",
        "        post = infer_parameter_posterior(\n",
        "            model=model,\n",
        "            normalizers=normalizers,\n",
        "            geometry=geometry,\n",
        "            observed_current=obs_current,\n",
        "            applied_signal=signal,\n",
        "            known_p_core=p_core_true[local_idx],\n",
        "            known_p_mask=known_mask,\n",
        "            obs_mask=obs_mask,\n",
        "            config=posterior_cfg,\n",
        "            seed=15000 + local_idx * 31 + (0 if split_name == 'in_distribution' else 1),\n",
        "        )\n",
        "\n",
        "        true_norm = ((p_core_true[local_idx] - p_mean) / p_std)[:phys_dim_base]\n",
        "        post_mean_norm = np.asarray(post['posterior_mean_norm'], dtype=np.float32)[:phys_dim_base]\n",
        "        param_nrmse = float(np.sqrt(np.mean((post_mean_norm - true_norm) ** 2)))\n",
        "        rel = dict(post['reliability'])\n",
        "\n",
        "        row = {\n",
        "            'split': split_name,\n",
        "            'case_index': int(row_idx),\n",
        "            'param_nrmse': param_nrmse,\n",
        "            'reliability_score': float(rel['reliability_score']),\n",
        "            'pred_nrmse': float(rel['nrmse']),\n",
        "            'pred_nll': float(rel['nll']),\n",
        "            'pred_calibration_error': float(rel['calibration_error']),\n",
        "            'pred_sharpness': float(rel['sharpness']),\n",
        "        }\n",
        "        runs.append(row)\n",
        "\n",
        "        key_name = f'{split_name}_{int(row_idx)}'\n",
        "        examples[key_name] = {\n",
        "            'case_index': int(row_idx),\n",
        "            'split': split_name,\n",
        "            'observed_current': np.asarray(post['observed_current'], dtype=np.float32),\n",
        "            'observed_mask': np.asarray(post['observed_mask'], dtype=np.float32),\n",
        "            'pred_mean': np.asarray(post['predictive_mean_current'], dtype=np.float32),\n",
        "            'pred_std': np.asarray(post['predictive_std_current'], dtype=np.float32),\n",
        "            'reliability_score': float(rel['reliability_score']),\n",
        "        }\n",
        "\n",
        "\n",
        "def summarize_split(name: str) -> dict[str, float | str]:\n",
        "    rows = [r for r in runs if r['split'] == name]\n",
        "    arr = lambda key: np.asarray([float(r[key]) for r in rows], dtype=float)\n",
        "    return {\n",
        "        'split': name,\n",
        "        'n_cases': float(len(rows)),\n",
        "        'reliability_score_mean': float(np.mean(arr('reliability_score'))),\n",
        "        'param_nrmse_mean': float(np.mean(arr('param_nrmse'))),\n",
        "        'pred_nrmse_mean': float(np.mean(arr('pred_nrmse'))),\n",
        "        'pred_nll_mean': float(np.mean(arr('pred_nll'))),\n",
        "        'pred_calibration_error_mean': float(np.mean(arr('pred_calibration_error'))),\n",
        "    }\n",
        "\n",
        "\n",
        "split_summary = [summarize_split('in_distribution'), summarize_split('stress_biofouled')]\n",
        "print(split_summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Reliability separation and threshold-based guardrail analysis\n",
        "id_rows = [r for r in runs if r['split'] == 'in_distribution']\n",
        "ood_rows = [r for r in runs if r['split'] == 'stress_biofouled']\n",
        "\n",
        "id_rel = np.asarray([float(r['reliability_score']) for r in id_rows], dtype=float)\n",
        "ood_rel = np.asarray([float(r['reliability_score']) for r in ood_rows], dtype=float)\n",
        "id_cal = np.asarray([float(r['pred_calibration_error']) for r in id_rows], dtype=float)\n",
        "ood_cal = np.asarray([float(r['pred_calibration_error']) for r in ood_rows], dtype=float)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4.5))\n",
        "axes[0].boxplot([id_rel, ood_rel], labels=['in_distribution', 'stress_biofouled'])\n",
        "axes[0].set_title('Reliability Score Separation')\n",
        "axes[0].set_ylabel('Score (0-100)')\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "axes[1].boxplot([id_cal, ood_cal], labels=['in_distribution', 'stress_biofouled'])\n",
        "axes[1].set_title('Calibration Error Separation')\n",
        "axes[1].set_ylabel('|cov1-0.6827| + |cov2-0.9545|')\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "fig.tight_layout()\n",
        "fig.savefig(ARTIFACT_DIR / 'posterior_failure_mode_separation.png', dpi=180)\n",
        "plt.show()\n",
        "\n",
        "y_true = np.asarray([0] * len(id_rows) + [1] * len(ood_rows), dtype=np.int32)\n",
        "rel_all = np.asarray([float(r['reliability_score']) for r in id_rows + ood_rows], dtype=float)\n",
        "cal_all = np.asarray([float(r['pred_calibration_error']) for r in id_rows + ood_rows], dtype=float)\n",
        "\n",
        "best = None\n",
        "for rel_thr in np.linspace(30.0, 85.0, 23):\n",
        "    for cal_thr in np.linspace(0.08, 0.60, 27):\n",
        "        flagged = (rel_all < rel_thr) | (cal_all > cal_thr)\n",
        "        tp = float(np.sum((flagged == 1) & (y_true == 1)))\n",
        "        fp = float(np.sum((flagged == 1) & (y_true == 0)))\n",
        "        tn = float(np.sum((flagged == 0) & (y_true == 0)))\n",
        "        fn = float(np.sum((flagged == 0) & (y_true == 1)))\n",
        "        precision = tp / max(tp + fp, 1e-9)\n",
        "        recall = tp / max(tp + fn, 1e-9)\n",
        "        f1 = 2.0 * precision * recall / max(precision + recall, 1e-9)\n",
        "        tpr = recall\n",
        "        fpr = fp / max(fp + tn, 1e-9)\n",
        "        candidate = {\n",
        "            'rel_thr': float(rel_thr),\n",
        "            'cal_thr': float(cal_thr),\n",
        "            'precision': float(precision),\n",
        "            'recall': float(recall),\n",
        "            'f1': float(f1),\n",
        "            'tpr': float(tpr),\n",
        "            'fpr': float(fpr),\n",
        "        }\n",
        "        if best is None or candidate['f1'] > best['f1']:\n",
        "            best = candidate\n",
        "\n",
        "print('best guardrail:', best)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Representative stress-case trace visualization\n",
        "stress_cases = [r for r in runs if r['split'] == 'stress_biofouled']\n",
        "worst = min(stress_cases, key=lambda r: float(r['reliability_score']))\n",
        "key = f\"stress_biofouled_{int(worst['case_index'])}\"\n",
        "ex = examples[key]\n",
        "\n",
        "obs = np.asarray(ex['observed_current'], dtype=float)\n",
        "mask = np.asarray(ex['observed_mask'], dtype=float) >= 0.5\n",
        "pred = np.asarray(ex['pred_mean'], dtype=float)\n",
        "std = np.maximum(np.asarray(ex['pred_std'], dtype=float), 1e-6)\n",
        "t = np.arange(obs.shape[0], dtype=float)\n",
        "\n",
        "plt.figure(figsize=(10, 4.5))\n",
        "plt.fill_between(t, pred - 2.0 * std, pred + 2.0 * std, alpha=0.22, label='pred \u00b12\u03c3')\n",
        "plt.fill_between(t, pred - std, pred + std, alpha=0.35, label='pred \u00b11\u03c3')\n",
        "plt.plot(t, pred, lw=1.25, label='pred mean')\n",
        "plt.plot(t[mask], obs[mask], 'k.', ms=3, label='observed')\n",
        "plt.plot(t[~mask], obs[~mask], '.', color='gray', ms=2, alpha=0.25, label='missing')\n",
        "plt.title(\n",
        "    f\"Worst stress case #{int(ex['case_index'])} | reliability={float(ex['reliability_score']):.2f}\"\n",
        ")\n",
        "plt.xlabel('Trace index')\n",
        "plt.ylabel('Current')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.legend(fontsize=8)\n",
        "plt.tight_layout()\n",
        "plt.savefig(ARTIFACT_DIR / 'posterior_worst_stress_trace.png', dpi=180)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Persist report payload\n",
        "payload = {\n",
        "    'config': {\n",
        "        'checkpoint': str(CHECKPOINT),\n",
        "        'dataset_chunk': str(DATASET_CHUNK),\n",
        "        'case_indices': case_indices.tolist(),\n",
        "        'eval_noise_std': EVAL_NOISE_STD,\n",
        "        'inference_config': INFERENCE_CFG,\n",
        "    },\n",
        "    'split_summary': split_summary,\n",
        "    'runs': runs,\n",
        "    'best_guardrail': best,\n",
        "}\n",
        "with open(ARTIFACT_DIR / 'posterior_failure_mode_report.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(payload, f, indent=2)\n",
        "\n",
        "payload['split_summary']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results and reviewer-facing interpretation\n",
        "\n",
        "- This notebook explicitly tests model mismatch, not only in-distribution success cases.\n",
        "- Reliability and calibration diagnostics are used as decision variables for accept/reject behavior.\n",
        "- The threshold rule provides an operational guardrail for production workflows when traces are likely outside training assumptions.\n",
        "\n",
        "## Improvement actions from this notebook\n",
        "\n",
        "- Gate downstream decisions on reliability diagnostics.\n",
        "- Route low-reliability or high-calibration-error traces to higher-fidelity multiphysics solvers or additional measurements.\n",
        "- Track guardrail metrics longitudinally as model/dataset revisions are introduced.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
