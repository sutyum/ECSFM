{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment: Nature Evidence 04 - Posterior Identifiability and Calibration\n",
        "\n",
        "Objective:\n",
        "- Validate latent-parameter posterior inference against held-out synthetic measurements with known ground truth.\n",
        "- Quantify both parameter recovery and predictive uncertainty calibration under realistic missingness/noise.\n",
        "- Produce reviewer-facing diagnostics that are falsifiable and reproducible.\n",
        "\n",
        "Success criteria:\n",
        "- Posterior predictive coverage tracks nominal Gaussian targets (1\u03c3 and 2\u03c3) on dense observations.\n",
        "- Parameter recovery degrades gracefully as observations become sparse/noisy.\n",
        "- Failure cases are explicitly visualized and interpreted.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Setup: imports and reproducibility\n",
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "import equinox as eqx\n",
        "import jax\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from ecsfm.fm.eval_classical import _resolve_model_geometry\n",
        "from ecsfm.fm.model import VectorFieldNet\n",
        "from ecsfm.fm.posterior import CEMPosteriorConfig, PosteriorInferenceConfig, infer_parameter_posterior\n",
        "from ecsfm.fm.train import (\n",
        "    MODEL_META_FILENAME,\n",
        "    NORMALIZERS_FILENAME,\n",
        "    load_model_metadata,\n",
        "    load_saved_normalizers,\n",
        ")\n",
        "\n",
        "np.random.seed(2026)\n",
        "\n",
        "ARTIFACT_DIR = Path('/tmp/ecsfm/notebook_nature_04')\n",
        "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print(f'Artifacts: {ARTIFACT_DIR}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plan\n",
        "\n",
        "- Load a trained surrogate and a held-out chunk.\n",
        "- Build pseudo-measurements from known trajectories under three observation regimes.\n",
        "- Infer posterior over unknown base electrochemical parameters while fixing known task/stage labels.\n",
        "- Quantify parameter error, posterior coverage, and predictive calibration.\n",
        "- Inspect worst-case traces and posterior marginals.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# User-facing configuration\n",
        "def _first_existing(candidates: list[Path]) -> Path | None:\n",
        "    for candidate in candidates:\n",
        "        if candidate.exists():\n",
        "            return candidate\n",
        "    return None\n",
        "\n",
        "\n",
        "def _discover_latest(pattern: str, roots: list[Path]) -> Path | None:\n",
        "    found: list[Path] = []\n",
        "    for root in roots:\n",
        "        if root.exists():\n",
        "            found.extend(root.rglob(pattern))\n",
        "    if not found:\n",
        "        return None\n",
        "    return max(found, key=lambda p: p.stat().st_mtime)\n",
        "\n",
        "\n",
        "checkpoint_override = os.getenv('ECSFM_CHECKPOINT')\n",
        "dataset_override = os.getenv('ECSFM_DATASET_CHUNK')\n",
        "\n",
        "checkpoint_candidates = [\n",
        "    Path('/tmp/ecsfm/fullscale_balanced_modal/surrogate_model.eqx'),\n",
        "    Path('/vol/artifacts/fullscale_balanced_modal/surrogate_model.eqx'),\n",
        "    Path('/tmp/ecsfm/surrogate_model.eqx'),\n",
        "]\n",
        "dataset_candidates = [\n",
        "    Path('/tmp/ecsfm/dataset_balanced_742k/chunk_0.npz'),\n",
        "    Path('/vol/datasets/dataset_balanced_742k/chunk_0.npz'),\n",
        "    Path('/tmp/ecsfm/dataset_massive/chunk_0.npz'),\n",
        "]\n",
        "\n",
        "if checkpoint_override:\n",
        "    CHECKPOINT = Path(checkpoint_override)\n",
        "else:\n",
        "    CHECKPOINT = _first_existing(checkpoint_candidates)\n",
        "    if CHECKPOINT is None:\n",
        "        CHECKPOINT = _discover_latest('surrogate_model.eqx', [Path('/tmp/ecsfm'), Path('/vol/artifacts')])\n",
        "\n",
        "if dataset_override:\n",
        "    DATASET_CHUNK = Path(dataset_override)\n",
        "else:\n",
        "    DATASET_CHUNK = _first_existing(dataset_candidates)\n",
        "    if DATASET_CHUNK is None:\n",
        "        DATASET_CHUNK = _discover_latest('chunk_0.npz', [Path('/tmp/ecsfm'), Path('/vol/datasets')])\n",
        "    if DATASET_CHUNK is None:\n",
        "        DATASET_CHUNK = _discover_latest('chunk_*.npz', [Path('/tmp/ecsfm'), Path('/vol/datasets')])\n",
        "\n",
        "if CHECKPOINT is None or not CHECKPOINT.exists():\n",
        "    raise FileNotFoundError(\n",
        "        'No checkpoint found. Set ECSFM_CHECKPOINT or place a model at one of: '\n",
        "        + ', '.join(str(p) for p in checkpoint_candidates)\n",
        "    )\n",
        "if DATASET_CHUNK is None or not DATASET_CHUNK.exists():\n",
        "    raise FileNotFoundError(\n",
        "        'No dataset chunk found. Set ECSFM_DATASET_CHUNK or place a chunk at one of: '\n",
        "        + ', '.join(str(p) for p in dataset_candidates)\n",
        "    )\n",
        "\n",
        "N_CASES = 8\n",
        "OBSERVATION_REGIMES = {\n",
        "    'dense_clean': {'keep_prob': 1.00, 'noise_std': 0.02},\n",
        "    'sparse_clean': {'keep_prob': 0.35, 'noise_std': 0.02},\n",
        "    'sparse_noisy': {'keep_prob': 0.35, 'noise_std': 0.08},\n",
        "}\n",
        "INFERENCE_BASE = {\n",
        "    'n_particles': 64,\n",
        "    'n_iters': 4,\n",
        "    'elite_frac': 0.25,\n",
        "    'n_mc': 2,\n",
        "    'n_steps': 60,\n",
        "}\n",
        "\n",
        "print('checkpoint:', CHECKPOINT)\n",
        "print('dataset chunk:', DATASET_CHUNK)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Load model + dataset and assemble true conditioning vectors\n",
        "normalizers = load_saved_normalizers(CHECKPOINT.parent / NORMALIZERS_FILENAME)\n",
        "meta = load_model_metadata(CHECKPOINT.parent / MODEL_META_FILENAME)\n",
        "geometry = _resolve_model_geometry(normalizers, meta)\n",
        "\n",
        "key = jax.random.PRNGKey(np.uint32(2026))\n",
        "_, model_key = jax.random.split(key)\n",
        "model = VectorFieldNet(\n",
        "    state_dim=int(geometry['state_dim']),\n",
        "    hidden_size=int(meta.get('hidden_size', 128)),\n",
        "    depth=int(meta.get('depth', 3)),\n",
        "    cond_dim=int(meta.get('cond_dim', 32)),\n",
        "    phys_dim=int(geometry['phys_dim']),\n",
        "    signal_channels=int(geometry['signal_channels']),\n",
        "    key=model_key,\n",
        ")\n",
        "model = eqx.tree_deserialise_leaves(CHECKPOINT, model)\n",
        "\n",
        "with np.load(DATASET_CHUNK) as chunk:\n",
        "    currents = np.asarray(chunk['i'], dtype=np.float32)\n",
        "    signals = np.asarray(chunk['e'], dtype=np.float32)\n",
        "    params_base = np.asarray(chunk['p'], dtype=np.float32)\n",
        "    task_ids = np.asarray(chunk['task_id'], dtype=np.int32)\n",
        "    stage_ids = np.asarray(chunk['stage_id'], dtype=np.int32)\n",
        "\n",
        "n_rows = currents.shape[0]\n",
        "rng = np.random.default_rng(2026)\n",
        "case_indices = np.asarray(\n",
        "    rng.choice(n_rows, size=min(N_CASES, n_rows), replace=False),\n",
        "    dtype=np.int32,\n",
        ")\n",
        "\n",
        "phys_dim_base = int(geometry['phys_dim_base'])\n",
        "phys_dim_core = int(geometry['phys_dim_core'])\n",
        "n_tasks = int(geometry['n_tasks'])\n",
        "n_stages = int(geometry['n_stages'])\n",
        "\n",
        "\n",
        "def compose_core_params(base_row: np.ndarray, task_idx: int, stage_idx: int) -> np.ndarray:\n",
        "    out = np.zeros((phys_dim_core,), dtype=np.float32)\n",
        "    out[:phys_dim_base] = base_row[:phys_dim_base]\n",
        "    cursor = phys_dim_base\n",
        "    if n_tasks > 0:\n",
        "        onehot = np.zeros((n_tasks,), dtype=np.float32)\n",
        "        onehot[int(np.clip(task_idx, 0, n_tasks - 1))] = 1.0\n",
        "        out[cursor : cursor + n_tasks] = onehot\n",
        "        cursor += n_tasks\n",
        "    if n_stages > 0:\n",
        "        onehot = np.zeros((n_stages,), dtype=np.float32)\n",
        "        onehot[int(np.clip(stage_idx, 0, n_stages - 1))] = 1.0\n",
        "        out[cursor : cursor + n_stages] = onehot\n",
        "    return out\n",
        "\n",
        "\n",
        "p_core_true = np.stack(\n",
        "    [compose_core_params(params_base[i], int(task_ids[i]), int(stage_ids[i])) for i in case_indices],\n",
        "    axis=0,\n",
        ")\n",
        "\n",
        "x_mean, x_std, e_mean, e_std, p_mean, p_std = normalizers\n",
        "p_mean = np.asarray(p_mean, dtype=np.float32)\n",
        "p_std = np.asarray(p_std, dtype=np.float32)\n",
        "\n",
        "print('selected cases:', case_indices.tolist())\n",
        "print(\n",
        "    f\"geometry: max_species={geometry['max_species']} nx={geometry['nx']} target_len={geometry['target_len']} \"\n",
        "    f\"phys_dim_base={phys_dim_base} phys_dim_core={phys_dim_core}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Posterior sweep across observation regimes\n",
        "def build_mask(length: int, keep_prob: float, rng: np.random.Generator, min_points: int = 24) -> np.ndarray:\n",
        "    if keep_prob >= 0.999:\n",
        "        return np.ones((length,), dtype=np.float32)\n",
        "    mask = rng.random(length) < keep_prob\n",
        "    if int(mask.sum()) < min_points:\n",
        "        idx = rng.choice(length, size=min(min_points, length), replace=False)\n",
        "        mask[idx] = True\n",
        "    return mask.astype(np.float32)\n",
        "\n",
        "\n",
        "records: list[dict[str, float | int | str]] = []\n",
        "zscores_by_regime: dict[str, np.ndarray] = {}\n",
        "example_payloads: dict[str, dict[str, np.ndarray | float | int]] = {}\n",
        "\n",
        "known_mask = np.zeros((phys_dim_core,), dtype=bool)\n",
        "known_mask[phys_dim_base:] = True\n",
        "\n",
        "for regime_idx, (regime_name, regime_cfg) in enumerate(OBSERVATION_REGIMES.items()):\n",
        "    regime_rows = []\n",
        "    regime_z = []\n",
        "    regime_payload = []\n",
        "\n",
        "    for local_case_idx, row_idx in enumerate(case_indices):\n",
        "        case_rng = np.random.default_rng(2026 + regime_idx * 10000 + int(row_idx))\n",
        "        signal = np.asarray(signals[row_idx], dtype=np.float32)\n",
        "        truth_current = np.asarray(currents[row_idx], dtype=np.float32)\n",
        "        noisy_current = truth_current + case_rng.normal(\n",
        "            0.0,\n",
        "            float(regime_cfg['noise_std']),\n",
        "            size=truth_current.shape[0],\n",
        "        ).astype(np.float32)\n",
        "        obs_mask = build_mask(\n",
        "            truth_current.shape[0],\n",
        "            keep_prob=float(regime_cfg['keep_prob']),\n",
        "            rng=case_rng,\n",
        "        )\n",
        "\n",
        "        posterior_cfg = PosteriorInferenceConfig(\n",
        "            cem=CEMPosteriorConfig(\n",
        "                n_particles=int(INFERENCE_BASE['n_particles']),\n",
        "                n_iterations=int(INFERENCE_BASE['n_iters']),\n",
        "                elite_fraction=float(INFERENCE_BASE['elite_frac']),\n",
        "            ),\n",
        "            n_mc_per_particle=int(INFERENCE_BASE['n_mc']),\n",
        "            n_integration_steps=int(INFERENCE_BASE['n_steps']),\n",
        "            obs_noise_std=float(regime_cfg['noise_std']),\n",
        "        )\n",
        "\n",
        "        t0 = time.perf_counter()\n",
        "        post = infer_parameter_posterior(\n",
        "            model=model,\n",
        "            normalizers=normalizers,\n",
        "            geometry=geometry,\n",
        "            observed_current=noisy_current,\n",
        "            applied_signal=signal,\n",
        "            known_p_core=p_core_true[local_case_idx],\n",
        "            known_p_mask=known_mask,\n",
        "            obs_mask=obs_mask,\n",
        "            config=posterior_cfg,\n",
        "            seed=2026 + regime_idx * 1000 + local_case_idx,\n",
        "        )\n",
        "        elapsed_s = float(time.perf_counter() - t0)\n",
        "\n",
        "        true_norm = ((p_core_true[local_case_idx] - p_mean) / p_std)[:phys_dim_base]\n",
        "        post_mean_norm = np.asarray(post['posterior_mean_norm'], dtype=np.float32)[:phys_dim_base]\n",
        "        post_std_norm = np.asarray(post['posterior_std_norm'], dtype=np.float32)[:phys_dim_base]\n",
        "        post_std_norm = np.maximum(post_std_norm, 1e-6)\n",
        "\n",
        "        param_nrmse = float(np.sqrt(np.mean((post_mean_norm - true_norm) ** 2)))\n",
        "        param_mae = float(np.mean(np.abs(post_mean_norm - true_norm)))\n",
        "        param_cov1 = float(np.mean(np.abs(post_mean_norm - true_norm) <= post_std_norm))\n",
        "        param_cov2 = float(np.mean(np.abs(post_mean_norm - true_norm) <= 2.0 * post_std_norm))\n",
        "\n",
        "        rel = dict(post['reliability'])\n",
        "        pred_mean = np.asarray(post['predictive_mean_current'], dtype=np.float32)\n",
        "        pred_std = np.maximum(np.asarray(post['predictive_std_current'], dtype=np.float32), 1e-6)\n",
        "        obs_current = np.asarray(post['observed_current'], dtype=np.float32)\n",
        "        obs_mask_rs = np.asarray(post['observed_mask'], dtype=np.float32) >= 0.5\n",
        "        z = (obs_current[obs_mask_rs] - pred_mean[obs_mask_rs]) / pred_std[obs_mask_rs]\n",
        "\n",
        "        row = {\n",
        "            'regime': regime_name,\n",
        "            'case_index': int(row_idx),\n",
        "            'elapsed_s': elapsed_s,\n",
        "            'obs_fraction': float(np.mean(obs_mask_rs)),\n",
        "            'param_nrmse': param_nrmse,\n",
        "            'param_mae': param_mae,\n",
        "            'param_cov_1sigma': param_cov1,\n",
        "            'param_cov_2sigma': param_cov2,\n",
        "            'reliability_score': float(rel['reliability_score']),\n",
        "            'pred_nrmse': float(rel['nrmse']),\n",
        "            'pred_nll': float(rel['nll']),\n",
        "            'pred_cov_1sigma': float(rel['coverage_1sigma']),\n",
        "            'pred_cov_2sigma': float(rel['coverage_2sigma']),\n",
        "            'pred_calibration_error': float(rel['calibration_error']),\n",
        "            'pred_sharpness': float(rel['sharpness']),\n",
        "        }\n",
        "        regime_rows.append(row)\n",
        "        regime_z.append(z)\n",
        "        regime_payload.append(\n",
        "            {\n",
        "                'case_index': int(row_idx),\n",
        "                'reliability_score': float(rel['reliability_score']),\n",
        "                'observed_current': obs_current,\n",
        "                'observed_mask': obs_mask_rs.astype(np.float32),\n",
        "                'pred_mean': pred_mean,\n",
        "                'pred_std': pred_std,\n",
        "                'samples_raw': np.asarray(post['posterior_samples_raw'], dtype=np.float32),\n",
        "                'weights': np.asarray(post['posterior_weights'], dtype=np.float32),\n",
        "                'post_mean_raw': np.asarray(post['posterior_mean_raw'], dtype=np.float32),\n",
        "            }\n",
        "        )\n",
        "\n",
        "    records.extend(regime_rows)\n",
        "    zscores_by_regime[regime_name] = np.concatenate(regime_z, axis=0)\n",
        "    worst_idx = int(np.argmin([row['reliability_score'] for row in regime_rows]))\n",
        "    example_payloads[regime_name] = regime_payload[worst_idx]\n",
        "\n",
        "\n",
        "def summarize(rows: list[dict[str, float | int | str]]) -> dict[str, float]:\n",
        "    arr = lambda key: np.asarray([float(r[key]) for r in rows], dtype=float)\n",
        "    return {\n",
        "        'n_cases': float(len(rows)),\n",
        "        'obs_fraction_mean': float(np.mean(arr('obs_fraction'))),\n",
        "        'elapsed_s_mean': float(np.mean(arr('elapsed_s'))),\n",
        "        'param_nrmse_mean': float(np.mean(arr('param_nrmse'))),\n",
        "        'param_cov_1sigma_mean': float(np.mean(arr('param_cov_1sigma'))),\n",
        "        'param_cov_2sigma_mean': float(np.mean(arr('param_cov_2sigma'))),\n",
        "        'pred_nrmse_mean': float(np.mean(arr('pred_nrmse'))),\n",
        "        'pred_nll_mean': float(np.mean(arr('pred_nll'))),\n",
        "        'pred_cov_1sigma_mean': float(np.mean(arr('pred_cov_1sigma'))),\n",
        "        'pred_cov_2sigma_mean': float(np.mean(arr('pred_cov_2sigma'))),\n",
        "        'pred_calibration_error_mean': float(np.mean(arr('pred_calibration_error'))),\n",
        "        'reliability_score_mean': float(np.mean(arr('reliability_score'))),\n",
        "    }\n",
        "\n",
        "\n",
        "summary = {\n",
        "    name: summarize([r for r in records if r['regime'] == name])\n",
        "    for name in OBSERVATION_REGIMES\n",
        "}\n",
        "\n",
        "print('--- posterior validation summary ---')\n",
        "for name, row in summary.items():\n",
        "    print(\n",
        "        f\"{name:12s} \"\n",
        "        f\"R={row['reliability_score_mean']:.2f} \"\n",
        "        f\"param_nrmse={row['param_nrmse_mean']:.3f} \"\n",
        "        f\"pred_nrmse={row['pred_nrmse_mean']:.3f} \"\n",
        "        f\"cal={row['pred_calibration_error_mean']:.3f} \"\n",
        "        f\"time={row['elapsed_s_mean']:.2f}s\"\n",
        "    )\n",
        "\n",
        "payload = {\n",
        "    'config': {\n",
        "        'checkpoint': str(CHECKPOINT),\n",
        "        'dataset_chunk': str(DATASET_CHUNK),\n",
        "        'case_indices': case_indices.tolist(),\n",
        "        'observation_regimes': OBSERVATION_REGIMES,\n",
        "        'inference_base': INFERENCE_BASE,\n",
        "    },\n",
        "    'summary': summary,\n",
        "    'records': records,\n",
        "}\n",
        "with open(ARTIFACT_DIR / 'posterior_identifiability_calibration.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(payload, f, indent=2)\n",
        "\n",
        "summary\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Calibration and recovery visualizations\n",
        "regime_names = list(OBSERVATION_REGIMES.keys())\n",
        "\n",
        "def _values(metric: str) -> list[np.ndarray]:\n",
        "    return [\n",
        "        np.asarray(\n",
        "            [float(r[metric]) for r in records if r['regime'] == name],\n",
        "            dtype=float,\n",
        "        )\n",
        "        for name in regime_names\n",
        "    ]\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4.5))\n",
        "\n",
        "axes[0].boxplot(_values('reliability_score'), labels=regime_names)\n",
        "axes[0].set_title('Posterior Reliability Score')\n",
        "axes[0].set_ylabel('Score (0-100)')\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "axes[1].boxplot(_values('param_nrmse'), labels=regime_names)\n",
        "axes[1].set_title('Parameter Recovery Error')\n",
        "axes[1].set_ylabel('NRMSE (normalized parameter space)')\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "cal_err = [\n",
        "    np.mean(np.asarray([float(r['pred_calibration_error']) for r in records if r['regime'] == name]))\n",
        "    for name in regime_names\n",
        "]\n",
        "axes[2].bar(np.arange(len(regime_names)), cal_err)\n",
        "axes[2].set_xticks(np.arange(len(regime_names)))\n",
        "axes[2].set_xticklabels(regime_names)\n",
        "axes[2].set_title('Predictive Calibration Error')\n",
        "axes[2].set_ylabel('|cov1-0.6827| + |cov2-0.9545|')\n",
        "axes[2].grid(axis='y', alpha=0.3)\n",
        "\n",
        "fig.tight_layout()\n",
        "fig.savefig(ARTIFACT_DIR / 'posterior_recovery_calibration_boxes.png', dpi=180)\n",
        "plt.show()\n",
        "\n",
        "thresholds = np.linspace(0.5, 2.5, 9)\n",
        "expected = np.asarray([math.erf(t / math.sqrt(2.0)) for t in thresholds], dtype=float)\n",
        "\n",
        "plt.figure(figsize=(7.5, 5.5))\n",
        "plt.plot(thresholds, expected, 'k--', lw=1.3, label='ideal Gaussian coverage')\n",
        "for name in regime_names:\n",
        "    z = np.asarray(zscores_by_regime[name], dtype=float)\n",
        "    empirical = np.asarray([np.mean(np.abs(z) <= t) for t in thresholds], dtype=float)\n",
        "    plt.plot(thresholds, empirical, 'o-', label=name)\n",
        "\n",
        "plt.xlabel('z threshold')\n",
        "plt.ylabel('Empirical coverage P(|z| <= threshold)')\n",
        "plt.title('Posterior Predictive Calibration Curves')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(ARTIFACT_DIR / 'posterior_calibration_curves.png', dpi=180)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Worst-case inspection for sparse_noisy regime\n",
        "example = example_payloads['sparse_noisy']\n",
        "obs = np.asarray(example['observed_current'], dtype=float)\n",
        "mask = np.asarray(example['observed_mask'], dtype=float) >= 0.5\n",
        "pred = np.asarray(example['pred_mean'], dtype=float)\n",
        "std = np.asarray(example['pred_std'], dtype=float)\n",
        "t = np.arange(obs.shape[0], dtype=float)\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 4.5))\n",
        "\n",
        "ax = axes[0]\n",
        "ax.fill_between(t, pred - 2.0 * std, pred + 2.0 * std, alpha=0.25, label='pred \u00b12\u03c3')\n",
        "ax.fill_between(t, pred - std, pred + std, alpha=0.35, label='pred \u00b11\u03c3')\n",
        "ax.plot(t, pred, lw=1.2, label='pred mean')\n",
        "ax.plot(t[mask], obs[mask], 'k.', ms=3, label='observed')\n",
        "ax.plot(t[~mask], obs[~mask], '.', color='gray', ms=2, alpha=0.25, label='missing')\n",
        "ax.set_title(f\"Worst sparse_noisy case #{example['case_index']}\")\n",
        "ax.set_xlabel('Trace index')\n",
        "ax.set_ylabel('Current')\n",
        "ax.grid(alpha=0.3)\n",
        "ax.legend(fontsize=8)\n",
        "\n",
        "samples = np.asarray(example['samples_raw'], dtype=float)\n",
        "weights = np.asarray(example['weights'], dtype=float)\n",
        "weights = weights / max(np.sum(weights), 1e-12)\n",
        "post_mean_raw = np.asarray(example['post_mean_raw'], dtype=float)\n",
        "true_idx = int(np.where(case_indices == int(example['case_index']))[0][0])\n",
        "true_row = p_core_true[true_idx]\n",
        "\n",
        "max_species = int(geometry['max_species'])\n",
        "idx_e0 = min(4 * max_species, phys_dim_base - 1)\n",
        "idx_logk0 = min(5 * max_species, phys_dim_base - 1)\n",
        "\n",
        "axes[1].hist(samples[:, idx_e0], bins=24, weights=weights, density=True, alpha=0.7)\n",
        "axes[1].axvline(true_row[idx_e0], color='k', linestyle='--', label='truth')\n",
        "axes[1].axvline(post_mean_raw[idx_e0], color='tab:red', linestyle='-', label='posterior mean')\n",
        "axes[1].set_title(f'Posterior marginal: E0[0] (idx={idx_e0})')\n",
        "axes[1].set_xlabel('E0 value')\n",
        "axes[1].grid(alpha=0.3)\n",
        "axes[1].legend(fontsize=8)\n",
        "\n",
        "axes[2].hist(samples[:, idx_logk0], bins=24, weights=weights, density=True, alpha=0.7)\n",
        "axes[2].axvline(true_row[idx_logk0], color='k', linestyle='--', label='truth')\n",
        "axes[2].axvline(post_mean_raw[idx_logk0], color='tab:red', linestyle='-', label='posterior mean')\n",
        "axes[2].set_title(f'Posterior marginal: log(k0)[0] (idx={idx_logk0})')\n",
        "axes[2].set_xlabel('log(k0) value')\n",
        "axes[2].grid(alpha=0.3)\n",
        "axes[2].legend(fontsize=8)\n",
        "\n",
        "fig.tight_layout()\n",
        "fig.savefig(ARTIFACT_DIR / 'posterior_sparse_noisy_worst_case.png', dpi=180)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results and reviewer-facing interpretation\n",
        "\n",
        "- This notebook provides a direct inverse-problem validation loop: known truth -> partial observations -> posterior inference -> recovery/calibration metrics.\n",
        "- Dense measurements should approach nominal calibration and strongest parameter recovery.\n",
        "- Sparse/noisy conditions expose identifiability limits explicitly, including posterior broadening and degraded coverage.\n",
        "- The worst-case panel is included intentionally to prevent cherry-picking and document where the model remains weak.\n",
        "\n",
        "## Improvement actions from this notebook\n",
        "\n",
        "- Use reliability-aware acceptance criteria (not point-estimate error alone).\n",
        "- Prefer measurement designs that increase information density in dynamic waveform regions.\n",
        "- Increase posterior compute budget only where reliability diagnostics indicate under-convergence.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
